---
title: "Examen_a_casa_aplicacion_de_GMLR_Ibarra_try0"
author: "SIR"
date: "2023-11-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### UNAM Posgrado en Ingeniería 
### Dr. Wulfrano Gomez Gallardo
### Alumno: Ibarra Ramírez Sergio 
<br>

### Ejemplo de aplicación de modelos Lineales Generalizados (GLR Ó MCG) en el pronóstico de sueldos

<br>

#### El modelo General Lineal (GLR) o Modelo de Mínimos Cuadrados (MCG) GeneralizadoS 
<br>

En general el modelo MCG asume los siguientes enunciados como ciertos:

 A. Las variables xi predictoras son linealmente indpendientes entre si, por lo que la matriz X es de rango completo
 B. Los errores siguen una distribución normal 
 $${\varepsilon}\sim N(\mathbf{0}, \sigma^2\mathbf{I}_n)$$
 C. El valor esperado del error dado los valores de la matriz X es cero 
 $$\mathbb{E} \left( \boldsymbol{\varepsilon} | \mathbf{X}\right) = \boldsymbol{0}$$
 D. La varianza del error dado los valores de la matriz X puede calcularse como: 
 
 $$\mathbb{V}{\rm ar}\left( \boldsymbol{\varepsilon} | \mathbf{X} \right)= \mathbb{E} \left( \boldsymbol{\varepsilon} \boldsymbol{\varepsilon}^\top \right)$$
 es decir, 
 
$$ \mathbb{V}{\rm ar}\left( \boldsymbol{\varepsilon} | \mathbf{X} \right) = \mathbb{E} \left( \boldsymbol{\varepsilon} \boldsymbol{\varepsilon}^\top \right)= \boldsymbol{\Sigma} = \sigma_\epsilon^2 \mathbf{\Omega}$$


Donde ${\Omega}$ es una matriz cuadrada, simétrica y definida positiva de tamaño N × N. Este modelo permite que los errores sean heterocedásticos o autocorrelacionados (o ambos) y a menudo se denomina caso especial del modelo lineal generalizado (GLM).



Si ${\Omega}=I$, entonces el GMLR es simplemente el modelo de regresión lineal múltiple simple que discutimos al comienzo de este capítulo.


$$ {\Omega}= \begin{pmatrix}
1 & 0 & 0 & ..  0\\ 
0 &  1& 0 & ..  0\\
0 &  0& 1 & ..  0\\ 
0 & 0 & 0 &  ..  1 
\end{pmatrix}$$ 


Si ${\Omega}$ es diagonal con elementos diagonales no constantes, entonces los términos de error no están correlacionados pero son heterocedásticos.


$${\Omega}= \begin{pmatrix}
\omega_{1}  & 0 & 0 & ..  0\\ 
0 &  \omega_{2}& 0 & ..  0\\
0 &  0& \omega_{3} & ..  0\\ 
0 & 0 & 0 &  ..  \omega_{n} 
\end{pmatrix}$$


Si ${\Omega}$ no es diagonal, entonces $Cov(\varepsilon_{i}, \varepsilon_{j})={\Omega_{ij}\neq 0}$ En econometría, las matrices de covarianza no diagonales se encuentran con mayor frecuencia en los datos de series temporales, con correlaciones más altas para las observaciones más cercanas en el tiempo (generalmente cuando i y j difieren en 1 o 2).


Si ${\Omega}$ no es diagonal y los elementos diagonales son constantes, entonces los errores están autocorrelacionados y homocedásticos.

$${\Omega}= \begin{pmatrix}
\sigma^{2}  & \rho_{12} & \rho_{13} & ..\rho_{1N}\\ 
\rho_{21}  &  \sigma^{2}& \rho_{23}  & ..\rho_{2N} \\
\rho_{31}  &  \rho_{32} & \sigma^{2} & ..\rho_{3N} \\ 
\rho_{41}  &  \rho_{42}  & \rho_{43}  &  ..\sigma^{2} 
\end{pmatrix}$$



Si ${\Omega}$ no es diagonal y los elementos diagonales no son constantes, entonces los errores están autocorrelacionados y heterocedásticos.


$${\Omega}= \begin{pmatrix}
\omega_{1}  & \rho_{12} & \rho_{13} & ..\rho_{1N}\\ 
\rho_{21}  &  \omega_{2}& \rho_{23}  & ..\rho_{2N} \\
\rho_{31}  &  \rho_{32} & \omega_{3} & ..\rho_{3N} \\ 
\rho_{41}  &  \rho_{42}  & \rho_{43}  &  ..\omega_{N} 
\end{pmatrix}$$


Y donde los estimadores ${\beta}$ del modelo lineal genralizado (GLS o MCG) puede ser calculado como : 

$$\begin{aligned}
\widehat{\boldsymbol{\beta}_{GLS}} = \left( \mathbf{X}^\top \mathbf{\Omega}^{-1}  \mathbf{X}\right)^{-1} \mathbf{X}^\top \mathbf{\Omega}^{-1}  \mathbf{Y}
\end{aligned}$$

Y cuya función de minimización de error o función de costo a minimizar como función de los estimadores del modelo GLS es:

$$\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^\top \mathbf{\Omega}^{-1} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \rightarrow \min_{\boldsymbol{\beta}}$$


<br>

#### La prueba estadística F(Fisher) para evaluación de valor general constante para todos los elementos del vecotr ${\beta}$

Donde la H0: El vector ${\beta}$ es igual a un valor determinado para todos sus elementos, por ejemplo ${\beta}=r$ donde r=0

$$\begin{cases}
H_0&: \boldsymbol{\beta} = \boldsymbol{r}\\
H_1&: \boldsymbol{\beta} \neq \boldsymbol{r}
\end{cases}$$


Donde se asume que ambos errores a) Error explicado por el modelo con k grados de libertad / b) Error no explicado por el modelo (error estocástico) con n-k grados de libertad siguen una distribución t de student para una determinada muestra y por lo tanto, el cociente de ambas cantidades pueden intepretarse como un estadístico F de fisher con K,N-K grados de libertado: 


$$F = \dfrac{ \left(\widehat{\boldsymbol{\beta}}_{GLS} - \boldsymbol{r} \right)^\top \left[\left( \mathbf{X}^\top \mathbf{\Omega}^{-1} \mathbf{X}  \right)\right]^{-1} \left( \widehat{\boldsymbol{\beta}}_{GLS} - \boldsymbol{r} \right)}{\widehat{\epsilon}^2}  \sim F_{(K, N - K)} $$

#### Las pruebas estadística T(student) para evaluación de valor individual de cada  elemento del vecotr ${\beta}$

Donde la H0: El i-esimo elemento del vector ${\beta}$ es igual a un valor determinado , por ejemplo ${\beta_{i}}=r$ donde puede ser cero r=0 ç


$$\begin{cases}
H_0&: \boldsymbol{\beta_{j}} = \boldsymbol{r}\\
H_1&: \boldsymbol{\beta_{j}} \neq \boldsymbol{r}
\end{cases}$$


Si $\gamma_{j}$ es el j-esimo coeficiente diagonal de la matriz  $\left ( X^{T} X \right )^{-1}$ y $\gamma_{j}>0$

Entonces el valor de t estadistico de prueba para cada $\beta_{j}$ es: 

$$t_{n-k}=\frac{\widehat\beta_{j} - \beta_{j} }{\sqrt{\sigma^{2} \gamma_{j}}}$$




<br>


#### El modelo General Lineal Simple o Modelo de Mínimos Cuadrados Ordinarios 

Consideremos al modelo de regresión lineal simple o modelo de Mínimos Cuadrados Ordinarios (OMS ó MCO) como: 

$$\begin{align}
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{align}$$


##### Donde: <br>
Y: Vector de valores estimados <br>
X: Matriz ampliada de valores de las variables independientes xi <br>
$\beta$: Vector de estimadores de maxima verosimilitud calculado con el modelo MCO 


El cual asume: 

$$Y |X \sim \operatorname{N}(\mu(X), \sigma^2I)$$ 

##### Donde: <br>
Y: Vector de valores estimados <br>
X: Matriz ampliada de valores de las variables independientes xi <br>
$\mu(X)$ is the mean of the normal distribution, which is a function of X  <br>
$\sigma^2I$ is the variance of the normal distribution, where I is the identity matrix.

Y  por lo tanto: 

$$E(Y |X) = \mu(X) = \mathbf{X}^\top\boldsymbol{\beta}$$



### Ejemplo de aplicación de GLM 



```{r}

library(lmtest)
library(lrmest)
library(tseries)
library(nortest)
##library(car)
library(sandwich)
library(lattice)
library(viridisLite)
library(leaps)

```


#### Importación y lectura de la data para el modelo GLM

```{r}

wage_source <- "http://www.principlesofeconometrics.com/poe5/data/csv/cps5_small.csv"
wage_data <- read.csv(file = wage_source, sep = ",", dec = ".", header = TRUE)
print(head(wage_data, 10))

```


Definiciones de variables:

black - = 1 si es negro
educ - años de educación
exper - experiencia potencial = edad - educación - 6
faminc - otros ingresos familiares, $
female - = 1 si es mujer
metro - = 1 si se encuentra en un área metropolitana
midwest - = 1 si es región del medio oeste
south - = 1 si es región sur
wage - ingresos por hora, $
west - = 1 si es región oeste



Tengamos un vistazo rápido  de los datos con un reumen estadístico de las variables 

```{r}
print(summary(wage_data))
```


#### División de la data en train & test

```{r}

dt_index <- 1:nrow(wage_data)
print(head(dt_index, 5))

```





https://ocw.mit.edu/courses/18-650-statistics-for-applications-fall-2016/2c0395a301a2ca798b1c0ee18cf6eedc_MIT18_650F16_Regression.pdf

https://ocw.mit.edu/courses/18-650-statistics-for-applications-fall-2016/dff89368051a5feae72b39c6422d0752_MIT18_650F16_GLM.pdf

https://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/4-6-Multiple-GLS.html


