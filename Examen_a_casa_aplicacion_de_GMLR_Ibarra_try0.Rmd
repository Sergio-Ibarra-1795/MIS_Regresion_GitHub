---
title: "Examen_a_casa_aplicacion_de_GMLR_Ibarra_try0"
author: "SIR"
date: "2023-11-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### UNAM Posgrado en Ingeniería 
### Dr. Wulfrano Gomez Gallardo
### Alumno: Ibarra Ramírez Sergio 
<br>

### Ejemplo de aplicación de modelos Lineales Generalizados (GLR Ó MCG) en el pronóstico de sueldos

<br>

#### El modelo General Lineal (GLR) o Modelo de Mínimos Cuadrados (MCG) GeneralizadoS 
<br>

En general el modelo MCG asume los siguientes enunciados como ciertos:

 A. Las variables xi predictoras son linealmente indpendientes entre si, por lo que la matriz X es de rango completo
 B. Los errores siguen una distribución normal 
 $${\varepsilon}\sim N(\mathbf{0}, \sigma^2\mathbf{I}_n)$$
 C. El valor esperado del error dado los valores de la matriz X es cero 
 $$\mathbb{E} \left( \boldsymbol{\varepsilon} | \mathbf{X}\right) = \boldsymbol{0}$$
 D. La varianza del error dado los valores de la matriz X puede calcularse como: 
 
 $$\mathbb{V}{\rm ar}\left( \boldsymbol{\varepsilon} | \mathbf{X} \right)= \mathbb{E} \left( \boldsymbol{\varepsilon} \boldsymbol{\varepsilon}^\top \right)$$
 es decir, 
 
$$ \mathbb{V}{\rm ar}\left( \boldsymbol{\varepsilon} | \mathbf{X} \right) = \mathbb{E} \left( \boldsymbol{\varepsilon} \boldsymbol{\varepsilon}^\top \right)= \boldsymbol{\Sigma} = \sigma_\epsilon^2 \mathbf{\Omega}$$


Donde ${\Omega}$ es una matriz cuadrada, simétrica y definida positiva de tamaño N × N. Este modelo permite que los errores sean heterocedásticos o autocorrelacionados (o ambos) y a menudo se denomina caso especial del modelo lineal generalizado (GLM).



Si ${\Omega}=I$, entonces el GMLR es simplemente el modelo de regresión lineal múltiple simple que discutimos al comienzo de este capítulo.


$$ {\Omega}= \begin{pmatrix}
1 & 0 & 0 & ..  0\\ 
0 &  1& 0 & ..  0\\
0 &  0& 1 & ..  0\\ 
0 & 0 & 0 &  ..  1 
\end{pmatrix}$$ 


Si ${\Omega}$ es diagonal con elementos diagonales no constantes, entonces los términos de error no están correlacionados pero son heterocedásticos.


$${\Omega}= \begin{pmatrix}
\omega_{1}  & 0 & 0 & ..  0\\ 
0 &  \omega_{2}& 0 & ..  0\\
0 &  0& \omega_{3} & ..  0\\ 
0 & 0 & 0 &  ..  \omega_{n} 
\end{pmatrix}$$


Si ${\Omega}$ no es diagonal, entonces $Cov(\varepsilon_{i}, \varepsilon_{j})={\Omega_{ij}\neq 0}$ En econometría, las matrices de covarianza no diagonales se encuentran con mayor frecuencia en los datos de series temporales, con correlaciones más altas para las observaciones más cercanas en el tiempo (generalmente cuando i y j difieren en 1 o 2).


Si ${\Omega}$ no es diagonal y los elementos diagonales son constantes, entonces los errores están autocorrelacionados y homocedásticos.

$${\Omega}= \begin{pmatrix}
\sigma^{2}  & \rho_{12} & \rho_{13} & ..\rho_{1N}\\ 
\rho_{21}  &  \sigma^{2}& \rho_{23}  & ..\rho_{2N} \\
\rho_{31}  &  \rho_{32} & \sigma^{2} & ..\rho_{3N} \\ 
\rho_{41}  &  \rho_{42}  & \rho_{43}  &  ..\sigma^{2} 
\end{pmatrix}$$



Si ${\Omega}$ no es diagonal y los elementos diagonales no son constantes, entonces los errores están autocorrelacionados y heterocedásticos.


$${\Omega}= \begin{pmatrix}
\omega_{1}  & \rho_{12} & \rho_{13} & ..\rho_{1N}\\ 
\rho_{21}  &  \omega_{2}& \rho_{23}  & ..\rho_{2N} \\
\rho_{31}  &  \rho_{32} & \omega_{3} & ..\rho_{3N} \\ 
\rho_{41}  &  \rho_{42}  & \rho_{43}  &  ..\omega_{N} 
\end{pmatrix}$$


Y donde los estimadores ${\beta}$ del modelo lineal genralizado (GLS o MCG) puede ser calculado como : 

$$\begin{aligned}
\widehat{\boldsymbol{\beta}_{GLS}} = \left( \mathbf{X}^\top \mathbf{\Omega}^{-1}  \mathbf{X}\right)^{-1} \mathbf{X}^\top \mathbf{\Omega}^{-1}  \mathbf{Y}
\end{aligned}$$

Y cuya función de minimización de error o función de costo a minimizar como función de los estimadores del modelo GLS es:

$$\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^\top \mathbf{\Omega}^{-1} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \rightarrow \min_{\boldsymbol{\beta}}$$


<br>

#### La prueba estadística F(Fisher) para evaluación de valor general constante para todos los elementos del vecotr ${\beta}$

Donde la H0: El vector ${\beta}$ es igual a un valor determinado para todos sus elementos, por ejemplo ${\beta}=r$ donde r=0

$$\begin{cases}
H_0&: \boldsymbol{\beta} = \boldsymbol{r}\\
H_1&: \boldsymbol{\beta} \neq \boldsymbol{r}
\end{cases}$$


Donde se asume que ambos errores a) Error explicado por el modelo con k grados de libertad / b) Error no explicado por el modelo (error estocástico) con n-k grados de libertad siguen una distribución t de student para una determinada muestra y por lo tanto, el cociente de ambas cantidades pueden intepretarse como un estadístico F de fisher con K,N-K grados de libertado: 


$$F = \dfrac{ \left(\widehat{\boldsymbol{\beta}}_{GLS} - \boldsymbol{r} \right)^\top \left[\left( \mathbf{X}^\top \mathbf{\Omega}^{-1} \mathbf{X}  \right)\right]^{-1} \left( \widehat{\boldsymbol{\beta}}_{GLS} - \boldsymbol{r} \right)}{\widehat{\epsilon}^2}  \sim F_{(K, N - K)} $$

#### Las pruebas estadística T(student) para evaluación de valor individual de cada  elemento del vecotr ${\beta}$

Donde la H0: El i-esimo elemento del vector ${\beta}$ es igual a un valor determinado , por ejemplo ${\beta_{i}}=r$ donde puede ser cero r=0 ç


$$\begin{cases}
H_0&: \boldsymbol{\beta_{j}} = \boldsymbol{r}\\
H_1&: \boldsymbol{\beta_{j}} \neq \boldsymbol{r}
\end{cases}$$


Si $\gamma_{j}$ es el j-esimo coeficiente diagonal de la matriz  $\left ( X^{T} X \right )^{-1}$ y $\gamma_{j}>0$

Entonces el valor de t estadistico de prueba para cada $\beta_{j}$ es: 

$$t_{n-k}=\frac{\widehat\beta_{j} - \beta_{j} }{\sqrt{\sigma^{2} \gamma_{j}}}$$




<br>


#### El modelo General Lineal Simple o Modelo de Mínimos Cuadrados Ordinarios 

Consideremos al modelo de regresión lineal simple o modelo de Mínimos Cuadrados Ordinarios (OMS ó MCO) como: 

$$\begin{align}
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{align}$$


##### Donde: <br>
Y: Vector de valores estimados <br>
X: Matriz ampliada de valores de las variables independientes xi <br>
$\beta$: Vector de estimadores de maxima verosimilitud calculado con el modelo MCO 


El cual asume: 

$$Y |X \sim \operatorname{N}(\mu(X), \sigma^2I)$$ 

##### Donde: <br>
Y: Vector de valores estimados <br>
X: Matriz ampliada de valores de las variables independientes xi <br>
$\mu(X)$ is the mean of the normal distribution, which is a function of X  <br>
$\sigma^2I$ is the variance of the normal distribution, where I is the identity matrix.

Y  por lo tanto: 

$$E(Y |X) = \mu(X) = \mathbf{X}^\top\boldsymbol{\beta}$$



### Ejemplo de aplicación de GLM 



```{r}

library(lmtest)
library(lrmest)
library(tseries)
library(nortest)
##library(car)
library(sandwich)
library(lattice)
library(viridisLite)
library(leaps)

```


#### Importación y lectura de la data para el modelo GLM

```{r}

wage_source <- "http://www.principlesofeconometrics.com/poe5/data/csv/cps5_small.csv"
wage_data <- read.csv(file = wage_source, sep = ",", dec = ".", header = TRUE)
print(head(wage_data, 10))

```


Definiciones de variables:

black - = 1 si es negro
educ - años de educación
exper - experiencia potencial = edad - educación - 6
faminc - otros ingresos familiares, $
female - = 1 si es mujer
metro - = 1 si se encuentra en un área metropolitana
midwest - = 1 si es región del medio oeste
south - = 1 si es región sur
wage - ingresos por hora, $
west - = 1 si es región oeste



Tengamos un vistazo rápido  de los datos con un reumen estadístico de las variables 

```{r}
print(summary(wage_data))
```


#### División de la data en train & test

```{r}

# Set seed for reproducibility
set.seed(123)

# Create an index for splitting the data
index <- sample(2, nrow(wage_data), replace = TRUE, prob = c(0.8, 0.2))

# Split the data into training and test sets
wage_train_data <- wage_data[index == 1, ]
wage_test_data <- wage_data[index == 2, ]

print(head(wage_test_data))

```


####  A. Las variables xi predictoras son linealmente indpendientes entre si, por lo que la matriz X es de rango completo


```{r}
panel.hist <- function(x, ...){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE, breaks = 30)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, ...)
}
panel.abs_cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y, use = "complete.obs"))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = 2)
}
```



```{r}
pairs(wage_train_data[, c('educ','exper', 'faminc', 'wage')],
      diag.panel = panel.hist, 
      lower.panel = panel.smooth, #upper.panel = panel.cor, 
      col = "dodgerblue4", 
      pch = 21, 
      bg = adjustcolor("dodgerblue3", alpha = 0.2))
```


Los valores absolutos de la correlación son útiles para determinar la fuerza general de la relación lineal, independientemente de la dirección de la correlación.

También podemos estar interesados en verificar el mapa de calor de la matriz de correlación por separado 


```{r fig.width=8}
myPanel <- function(x, y, z, ...){
  lattice::panel.levelplot(x, y, round(z, 2), ...)  # Round the correlation values to two decimals
  my_text <- ifelse(!is.na(z), paste0(round(z, 2)), "")
  lattice::panel.text(x, y, my_text, cex = 0.8)  # Adjust the font size
}
# base colors: terrain.colors(), rainbow(), heat.colors(), topo.colors() or cm.colors()
# Viridis: viridis, magma, inferno, plasma
lattice::levelplot(cor(wage_train_data, use = "complete.obs"), panel = myPanel, 
                   col.regions = viridisLite::viridis(100),
  main =("Valor de Correlacion Lineal entre las variables Xi predictoras"), ylab = list(label = "Variables Xi predictoras"),
                               xlab = list(label = "Variables Xi predictoras"), scales = list(x = list(rot = 90)))  #)  # Rotate the x-axis labels by 90 degrees

```



Observamos queObservamos que wage y educ tienen una correlación positiva. Esta es la correlación positiva más fuerte en el conjunto de datos.
south, midwest y west parecen estar negativamente correlacionados, sin embargo, es lo que se espera - son variables indicadoras de la ubicación geográfica, por lo que una observación solo puede tener una de estas variables con un valor de 1
1 a la vez.



####  Aplicamos un modello lineal simple SLM o MCO a para encontrar el error y hacer las pruebas de las hipotesis planteadas como ciertas 



```{r}
# Load libraries
library(stats)

# Fit linear model
olm_model_wage_train_data <- lm(wage ~ black+educ+exper+faminc+female+metro+midwest+south, data = wage_train_data)

# View model summary
summary(olm_model_wage_train_data)
```

####  Interpretación un modelo lineal simple SLM o MCO para pronóstico de salario con base en datos raciales, de genero y geográficos


Según el modelo estimado, se observa lo siguiente:

Los coeficientes son los esperados para las variables "educ", "exper":
- Un año más de educación representa en promedio 2.54 más de ganancia 
- Un año más de experiencia representa en promedio 0.19 más de ganancia 

El coeficiente de la variable "metro" también es el esperado:
- Vivir en un área metropolitana representa en promedio 3.48 más de ganancia con respecto a no vivir en un área metropolitana 


Para los coeficientes regionales de las variables "west" y "midwest", dado que solo una de ellas es significativa o "linealmente independiente", el programa tomó solo una de ellas en la ecuación lineal del modelo MCO. 
- Vivir en un área "west" o "midwest" representa en promedio 1.4 menos de ganancia con respecto a no vivir en un área "west" o "midwest"


Para los coeficientes regionales de la variable "south"
- Vivir en un área "south" representa en promedio 1.22 menos de ganancia con respecto a no vivir en un área "south"


El signo del coeficiente de la variable "female" es negativo y significativo, lo que indica una posible discriminación en la fuerza laboral (cabe recalcar que este es solo el modelo inicial, por lo que aún no podemos estar seguros).
- Ser "Female" representa en promedio 5.7 menos de ganancia con respecto a no ser "Female"


El signo del coeficiente de la variable "black" es negativo, pero no tan grande como Female
- Ser "Black" representa en promedio 1.14 menos de ganancia con respecto a no ser "Black"


### Evaluación de las hipotesis que sustentan el modelo MCO


Queremos probar por separado la hipótesis de que cada coeficiente sea significativo a través de la prueba t de student que se presentó arriba


$$\begin{cases}
H_0&: \boldsymbol{\beta_{j}} = \boldsymbol{r}\\
H_1&: \boldsymbol{\beta_{j}} \neq \boldsymbol{r}
\end{cases}$$


```{r}
print(round(coef(summary(olm_model_wage_train_data)), 4))
```


### Modelo MCO2 (removiendo las variables que no resultaron significatrivas en el MCO original que incluía todas las variables predictoras xi del conjunto de datos como variables predictoras)

Comencemos por remover las variables cuyos coeficientes p en la prueba t sean mayores a 0.05, en este caso son (black, midwest y south)


```{r}
# Load libraries
library(stats)

# Fit linear model
olm_model2_wage_train_data <- lm(wage ~ educ+exper+faminc+female+metro, data = wage_train_data)

# View model summary
summary(olm_model2_wage_train_data)
```


#### Evaluación general de los error del modelo MCO


```{r}
# Define the plot layout matrix:
layout_mat <- matrix(c(1, 1, 2, 2, 3, 3, 3, 3), 
                     nrow = 2, byrow = TRUE)
# print(layout_mat)
layout(layout_mat)
plot(olm_model2_wage_train_data$fitted.values, olm_model2_wage_train_data$residuals, type = "p", 
     pch = 21, bg = "cornflowerblue", main = "Residuals vs Fitted", 
     ylab = "residuals", xlab = "fitted values", cex = 1.5)
hist(olm_model2_wage_train_data$residuals, col = "cornflowerblue", 
     breaks = 30, main = "Residual Histogram")
qqnorm(olm_model2_wage_train_data$residuals, main = "Q-Q plot of residuals", 
       pch = 21, bg = "cornflowerblue", cex = 1.5)
qqline(olm_model2_wage_train_data$residuals, col = "red", lwd = 2)
```


##### Test de Homocedasticidad de errores del modelo MCO ( Breusch-Pagan Test)

$$\begin{cases}
H_0&: \text{residuals are homoskedastic}\\
H_1&: \text{residuals are heteroskedastic}
\end{cases}$$


```{r}
# Breusch–Pagan Test

print(lmtest::bptest(olm_model2_wage_train_data))
```


En este caso el valor de p< 0.05, por lo tanto rechazamos la hipótesis nula de que los residuos son homocedásticos. Esto significa que los residuos son heterocedásticos.





##### Test de Autocorrelación de errores del modelo MCO (Durbin-Watson Test)

$$\begin{cases}
H_0&:\text{the errors are serially uncorrelated}\\
H_1&:\text{the errors are autocorrelated (the exact order of the autocorrelation depends on the test carried out)}
\end{cases}$$



```{r}

# Durbin–Watson Test
print(lmtest::dwtest(olm_model2_wage_train_data, alternative = "two.sided"))

```


El estadístico de DW está cerca de 2 (esto se puede verificar en R, ya que el valor p asociado es mayor a 0.05), por lo que no rechazamos la hipótesis nula de que no existe correlación serial.



##### Test de Autocorrelación de Parcial errores del modelo MCO (PACF)


```{r}
pacf(olm_model2_wage_train_data$residuals)
```

Como podemos ver, no tenemos motivos para rechazar la hipótesis nula de autocorrelación en ninguno de los casos (excepto quizá para los errores a lag 20 y 27)

A partir de estos resultados de la prueba, podemos concluir que los residuos no están autocorrelacionados



##### Test de Normalidad de los errores del modelo MCO (Jarque-Bera)

$$\begin{cases}
H_0&:\text{residuals follow a normal distribution}\\
H_1&:\text{residuals do not follow a normal distribution}
\end{cases}$$


```{r}
 tseries::jarque.bera.test(olm_model2_wage_train_data$residuals)
```


A partir de estos resultados de la prueba, podemos decir que tenemos suficiente información para rechazar H0 y por lo tanto concluir que los residuos NO se distribuyen normalmente :(



### Modelo MCO3 (removiendo las variables que no resultaron significatrivas en el MCO original que incluía todas las variables predictoras xi del conjunto de datos como variables predictoras + Logaritmo de variable dependiente wage como medida para lograr normalidad en error)

Comencemos por remover las variables cuyos coeficientes p en la prueba t sean mayores a 0.05, en este caso son (black, midwest y south)


```{r}
# Load libraries
library(stats)

# Fit linear model
olm_model3_wage_train_data <- lm(log(wage) ~ educ+exper+faminc+female+metro, data = wage_train_data)

# View model summary
summary(olm_model3_wage_train_data)
```


#### Evaluación general de los error del modelo MCO


```{r}
# Define the plot layout matrix:
layout_mat <- matrix(c(1, 1, 2, 2, 3, 3, 3, 3), 
                     nrow = 2, byrow = TRUE)
# print(layout_mat)
layout(layout_mat)
plot(olm_model3_wage_train_data$fitted.values, olm_model3_wage_train_data$residuals, type = "p", 
     pch = 21, bg = "cornflowerblue", main = "Residuals vs Fitted", 
     ylab = "residuals", xlab = "fitted values", cex = 1.5)
hist(olm_model3_wage_train_data$residuals, col = "cornflowerblue", 
     breaks = 30, main = "Residual Histogram")
qqnorm(olm_model3_wage_train_data$residuals, main = "Q-Q plot of residuals", 
       pch = 21, bg = "cornflowerblue", cex = 1.5)
qqline(olm_model3_wage_train_data$residuals, col = "red", lwd = 2)
```



```{r}
 tseries::jarque.bera.test(olm_model3_wage_train_data$residuals)
```





## Bibliografía 


https://ocw.mit.edu/courses/18-650-statistics-for-applications-fall-2016/2c0395a301a2ca798b1c0ee18cf6eedc_MIT18_650F16_Regression.pdf

https://ocw.mit.edu/courses/18-650-statistics-for-applications-fall-2016/dff89368051a5feae72b39c6422d0752_MIT18_650F16_GLM.pdf

https://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/4-6-Multiple-GLS.html


